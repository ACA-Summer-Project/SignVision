{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCKTNELCceox"
      },
      "outputs": [],
      "source": [
        "#Q1 : Neural Network with PyTorch - Classifying Iris Flowers\n",
        "#1.1: Load and Preprocess the Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('iris.csv')\n",
        "\n",
        "# Encode target column\n",
        "le = LabelEncoder()\n",
        "df['species'] = le.fit_transform(df['species'])  # setosa=0, versicolor=1, virginica=2\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop('species', axis=1).values\n",
        "y = df['species'].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor  = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test_tensor  = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader\n",
        "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_data  = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "test_loader  = DataLoader(test_data, batch_size=16)\n",
        "\n",
        "#1.2: Visualize the Data\n",
        "# Box plots\n",
        "features = df.columns[:-1]\n",
        "for feature in features:\n",
        "    plt.figure()\n",
        "    sns.boxplot(data=df, x='species', y=feature)\n",
        "    plt.title(f'{feature} distribution by species')\n",
        "    plt.xlabel('Species (0=setosa, 1=versicolor, 2=virginica)')\n",
        "    plt.show()\n",
        "\n",
        "# Optional: pairplot\n",
        "sns.pairplot(df, hue='species')\n",
        "plt.show()\n",
        "\n",
        "#1.3: Build the Neural Network\n",
        "import torch.nn as nn\n",
        "\n",
        "class IrisNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IrisNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, 3)  # 3 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = IrisNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "#1.4: Train the Model\n",
        "train_losses, test_losses = [], []\n",
        "train_accuracies, test_accuracies = [], []\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        train_loss, train_acc = evaluate(train_loader)\n",
        "        test_loss, test_acc = evaluate(test_loader)\n",
        "        print(f\"Epoch {epoch} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}\")\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        test_accuracies.append(test_acc)\n",
        "\n",
        "#1.5: Create Plots\n",
        "#1.5.1 - Loss vs Epochs\n",
        "epochs = range(10, 101, 10)\n",
        "plt.plot(epochs, train_losses, label='Train Loss')\n",
        "plt.plot(epochs, test_losses, label='Test Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#1.5.2 - Accuracy vs Epochs\n",
        "plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
        "plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#1.5.3 - Final Confusion Matrix\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "cm = confusion_matrix(y_test, preds.numpy())\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2 : Optical Extension - Understanding and Implementing Backpropagation from Scratch\n",
        "#2.1: Understand the Theory of Backpropagation\n",
        "#Forward Pass\n",
        "#Loss Calculation\n",
        "#Backward Pass\n",
        "#Weight Update Rule\n",
        "\n",
        "#2.2: Implement Backpropagation from Scratch\n",
        "import numpy as np\n",
        "\n",
        "# Sigmoid and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Loss Function: Binary Cross-Entropy\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    return -np.mean(y_true*np.log(y_pred+1e-8) + (1-y_true)*np.log(1-y_pred+1e-8))\n",
        "\n",
        "# Data: XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(2, 2)\n",
        "b1 = np.zeros((1, 2))\n",
        "W2 = np.random.randn(2, 1)\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "# Hyperparameters\n",
        "lr = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # FORWARD PASS\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # LOSS\n",
        "    loss = binary_cross_entropy(y, a2)\n",
        "\n",
        "    # BACKWARD PASS\n",
        "    d_loss_a2 = a2 - y                     # derivative of loss w.r.t a2\n",
        "    d_a2_z2 = sigmoid_derivative(z2)\n",
        "    d_z2_W2 = a1\n",
        "\n",
        "    dW2 = np.dot(d_z2_W2.T, d_loss_a2 * d_a2_z2)\n",
        "    db2 = np.sum(d_loss_a2 * d_a2_z2, axis=0, keepdims=True)\n",
        "\n",
        "    d_z2_a1 = W2\n",
        "    d_a1_z1 = sigmoid_derivative(z1)\n",
        "\n",
        "    dW1 = np.dot(X.T, (np.dot(d_loss_a2 * d_a2_z2, d_z2_a1.T) * d_a1_z1))\n",
        "    db1 = np.sum((np.dot(d_loss_a2 * d_a2_z2, d_z2_a1.T) * d_a1_z1), axis=0, keepdims=True)\n",
        "\n",
        "    # WEIGHT UPDATE\n",
        "    W1 -= lr * dW1\n",
        "    b1 -= lr * db1\n",
        "    W2 -= lr * dW2\n",
        "    b2 -= lr * db2\n",
        "\n",
        "    # Print loss occasionally\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "#2.3: Compare with PyTorchâ€™s Built-in Backpropagation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Data\n",
        "X_t = torch.tensor(X, dtype=torch.float32)\n",
        "y_t = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# Model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 2),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(2, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(10000):\n",
        "    # Forward\n",
        "    outputs = model(X_t)\n",
        "    loss = criterion(outputs, y_t)\n",
        "\n",
        "    # Backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "#If implemented correctly, both models will have similar results.\n",
        "\n",
        "#2.4: Experiment with Different Learning Rates\n",
        "#Change (lr = 0.1) to different values\n",
        "#lr = 0.01: slower learning, may not converge well\n",
        "#lr = 0.1: balanced\n",
        "#lr = 1.0: might overshoot or diverge\n",
        "#Observation -\n",
        "#Very small lr: slow convergence\n",
        "#Very large lr: possible divergence or oscillations"
      ],
      "metadata": {
        "id": "UlJpmeooeTqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}